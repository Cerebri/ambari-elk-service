<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->

<configuration>
  <!-- logstash.conf -->
  <property>
    <name>content</name>
    <description>This is logstash configuration file to define input, filter and output.</description>
    <value>
# pipeline of mapreduce finished jobs summary

input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/hadoop-mapreduce.jobsummary.log"
    sincedb_path => "{{logstash_sincedb_path}}/mapred.finished.jobsummary"
    start_position => beginning
    type => "mapred.finished"
  }
}

filter {
  if [type] == "mapred.finished" {
    grok {
      match => {   
        "message" => "\A%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+(?&lt;logger&gt;(?:[a-zA-Z0-9-]+\.)*[A-Za-z0-9$]+)\s*(:\s+)?appId=(?:%{WORD:appId}),name=(?&lt;name&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),user=(?&lt;user&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),queue=(?&lt;queue&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),state=(?:%{WORD:state}),trackingUrl=(?&lt;trackingUrl&gt;(?:.*)),appMasterHost=(?&lt;appMasterHost&gt;(?:.*)),startTime=(?:%{WORD:startTime}),finishTime=(?:%{WORD:finishTime}),finalStatus=(?:%{WORD:finalStatus}),memorySeconds=(?:%{WORD:memorySeconds}),vcoreSeconds=(?:%{WORD:vcoreSeconds}),preemptedAMContainers=(?:%{WORD:preemptedAMContainers}),preemptedNonAMContainers=(?:%{WORD:preemptedNonAMContainers}),preemptedResources=&lt;memory:(?:%{WORD:preemptedResources_memory})(\\,.*)vCores:(?:%{WORD:preemptedResources_vCores})&gt;,applicationType=(?:%{WORD:applicationType})"
      }
    }
    mutate {
      convert => ["startTime", "integer"]
      convert => ["finishTime", "integer"]
      convert => ["memorySeconds", "integer"]
      convert => ["vcoreSeconds", "integer"]
      convert => ["preemptedAMContainers", "integer"]
      convert => ["preemptedNonAMContainers", "integer"]
      convert => ["preemptedResources_memory", "integer"]
      convert => ["preemptedResources_vCores", "integer"]
    }
    ruby {
      code => "event['executionTime'] = event['finishTime']/1000 - event['startTime']/1000"
    }
    if ("_grokparsefailure" in [tags]) { drop {} }
  }
}

output {
  if [type] == "mapred.finished" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-mapred-finished"
    }
  }
}

# pipeline of mapreduce running jobs

input {
  exec {
    command => "if [ `hostname` == {{rm_host}} ]; then mapred job -list > {{logstash_log_dir}}/mapred.running.jobs; fi"
    interval => 30
  }
}

input {
  file {
    path => "{{logstash_log_dir}}/mapred.running.jobs"
    sincedb_path => "{{logstash_sincedb_path}}/mapred.running"
    type => "mapred.running"
  }
}

filter {
  if [type] == "mapred.running" {
    grok {
      match => {   
        "message" => "%{WORD:JobId}\s+%{WORD:State}\s+%{NUMBER:StartTime}\s+%{USER:UserName}\s+%{WORD:Queue}\s+%{WORD:Priority}\s+%{WORD:UsedContainers}\s+%{WORD:RsvdContainers}\s+%{NUMBER:UsedMem}M\s+%{WORD:RsvdMem}\s+%{WORD:NeededMem}\s+%{URI:AMinfo}"
      }
    }
    mutate {
      convert => ["StartTime", "integer"]
      convert => ["UsedMem", "integer"]
    }
    ruby {
      code => "
        time_in_event = Time.iso8601(event['@timestamp'].to_s).to_i;
        event['executionTime'] = time_in_event - event['StartTime']/1000
      "
    }
    if ("_grokparsefailure" in [tags]) { drop {} }
  }
}

output {
  if [type] == "mapred.running" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-mapred-running"
    }
  }
}

# pipeline of mapreduce aggregated memory usage per queue

input {
  file {
    path => "{{logstash_log_dir}}/mapred.running.jobs"
    sincedb_path => "{{logstash_sincedb_path}}/mapred.aggregated.queue"
    type => "mapred.aggregated.queue"
  }
}

filter {
  if [type] == "mapred.aggregated.queue" {
    grok {
      match => {   
        "message" => "%{WORD:JobId}\s+%{WORD:State}\s+%{NUMBER:StartTime}\s+%{USER:UserName}\s+%{WORD:Queue}\s+%{WORD:Priority}\s+%{WORD:UsedContainers}\s+%{WORD:RsvdContainers}\s+%{NUMBER:UsedMem}M\s+%{WORD:RsvdMem}\s+%{WORD:NeededMem}\s+%{URI:AMinfo}"
      }
    }
    mutate {
      convert => ["UsedMem", "integer"]
    }
    aggregate {
      task_id => "%{Queue}"
      code => "map['QueueUsedMem'] ||= 0 ; map['QueueUsedMem'] += event['UsedMem']"
      push_map_as_event_on_timeout => true
      timeout_task_id_field => "Queue"
      timeout => 5
      timeout_tags => ['aggregated.queue']
    }
    if "aggregated.queue" not in [tags] { drop {} }
  }
}

output {
  if "aggregated.queue" in [tags] {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-mapred-aggregated-queue"
    }
  }
}

# pipeline of mapreduce aggregated memory usage per user

input {
  file {
    path => "{{logstash_log_dir}}/mapred.running.jobs"
    sincedb_path => "{{logstash_sincedb_path}}/mapred.aggregated.user"
    type => "mapred.aggregated.user"
  }
}

filter {
  if [type] == "mapred.aggregated.user" {
    grok {
      match => {   
        "message" => "%{WORD:JobId}\s+%{WORD:State}\s+%{NUMBER:StartTime}\s+%{USER:UserName}\s+%{WORD:Queue}\s+%{WORD:Priority}\s+%{WORD:UsedContainers}\s+%{WORD:RsvdContainers}\s+%{NUMBER:UsedMem}M\s+%{WORD:RsvdMem}\s+%{WORD:NeededMem}\s+%{URI:AMinfo}"
      }
    }
    mutate {
      convert => ["UsedMem", "integer"]
    }
    aggregate {
      task_id => "%{UserName}"
      code => "map['UserUsedMem'] ||= 0 ; map['UserUsedMem'] += event['UsedMem']"
      push_map_as_event_on_timeout => true
      timeout_task_id_field => "UserName"
      timeout => 5
      timeout_tags => ['aggregated.user']
    }
    if "aggregated.user" not in [tags] { drop {} }
  }
}

output {
  if "aggregated.user" in [tags] {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-mapred-aggregated-user"
    }
  }
}

#pipeline of yarn resource allocation
input {
  exec {
    command => "yes|cp /dev/null {{logstash_log_dir}}/yarn_allocation; python {{logstash_bin}}/yarn-capacity.py > {{logstash_log_dir}}/yarn_allocation"
    interval => 30
  }
}

input {
  file {
    path => "{{logstash_log_dir}}/yarn_allocation"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.allocation"
    type => "yarn.allocation"
  }
}

filter {
  if [type] == "yarn.allocation" {
    grok {
      match => {   
        "message" => "%{WORD:QueueName}\s+%{NUMBER:TotalMemory}\s+%{NUMBER:TotalvCores}\s+%{NUMBER:UsedMemory}\s+%{NUMBER:UsedvCores}"
      }
    }
    mutate {
      convert => ["TotalMemory", "integer"]
      convert => ["TotalvCores", "integer"]
      convert => ["UsedMemory", "integer"]
      convert => ["UsedvCores", "integer"]
    }
    if ("_grokparsefailure" in [tags]) { drop {} }
  }
}

output {
  if [type] == "yarn.allocation" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-yarn-allocation"
    }
  }
}

#pipeline of hdfs namenode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-namenode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.namenode.log"
    start_position => beginning
    type => "hdfs.namenode.log"
  }
}

filter {
  if [type] == "hdfs.namenode.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hdfs.namenode.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs-namenode-log"
    }
  }
}

#pipeline of hdfs secondarynamenode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-secondarynamenode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.secondarynamenode.log"
    start_position => beginning
    type => "hdfs.secondarynamenode.log"
  }
}

filter {
  if [type] == "hdfs.secondarynamenode.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hdfs.secondarynamenode.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs-secondarynamenode-log"
    }
  }
}

#pipeline of hdfs journalnode logs
input {
  file {
    path => "/{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-journalnode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.journalnode.log"
    start_position => beginning
    type => "hdfs.journalnode.log"
  }
}

filter {
  if [type] == "hdfs.journalnode.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hdfs.journalnode.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs-journalnode-log"
    }
  }
}

#pipeline of hdfs datanode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-datanode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.datanode.log"
    start_position => beginning
    type => "hdfs.datanode.log"
  }
}

filter {
  if [type] == "hdfs.datanode.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hdfs.datanode.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs-datanode-log"
    }
  }
}

#pipeline of yarn nodemanager logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-nodemanager-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.nodemanager.log"
    start_position => beginning
    type => "yarn.nodemanager.log"
  }
}

filter {
  if [type] == "yarn.nodemanager.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "yarn.nodemanager.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-yarn-nodemanager-log"
    }
  }
}

#pipeline of yarn resourcemanager logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-resourcemanager-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.resourcemanager.log"
    start_position => beginning
    type => "yarn.resourcemanager.log"
  }
}

filter {
  if [type] == "yarn.resourcemanager.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "yarn.resourcemanager.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-yarn-resourcemanager-log"
    }
  }
}

#pipeline of yarn timelineserver logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-timelineserver-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.timelineserver.log"
    start_position => beginning
    type => "yarn.timelineserver.log"
  }
}

filter {
  if [type] == "yarn.timelineserver.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "yarn.timelineserver.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-yarn-timelineserver-log"
    }
  }
}

#pipeline of hbase master logs
input {
  file {
    path => "{{hbase_log_dir}}/*-master-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hbase.master.log"
    start_position => beginning
    type => "hbase.master.log"
  }
}

filter {
  if [type] == "hbase.master.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hbase.master.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hbase-master-log"
    }
  }
}

#pipeline of hbase regionserver logs
input {
  file {
    path => "{{hbase_log_dir}}/*-regionserver-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hbase.regionserver.log"
    start_position => beginning
    type => "hbase.regionserver.log"
  }
}

filter {
  if [type] == "hbase.regionserver.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hbase.regionserver.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hbase-regionserver-log"
    }
  }
}

#pipeline of hbase phoenix server logs
input {
  file {
    path => "{{hbase_log_dir}}/*-phoenix-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/phoenix-*.log"
    start_position => beginning
    type => "hbase.phoenix.log"
  }
}

filter {
  if [type] == "hbase.phoenix.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hbase.phoenix.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hbase-phoenix-log"
    }
  }
}

#pipeline of zookeeper server logs
input {
  file {
    path => "{{zk_log_dir}}/*-server-*.out"
    sincedb_path => "{{logstash_sincedb_path}}/zookeeper.server.log"
    start_position => beginning
    type => "zookeeper.server.log"
  }
}

filter {
  if [type] == "zookeeper.server.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+-\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "zookeeper.server.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-zookeeper-server-log"
    }
  }
}

#pipeline of hive metastore logs
input {
  file {
    path => "{{hive_log_dir}}/hivemetastore.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.metastore.log"
    start_position => beginning
    type => "hive.metastore.log"
  }
}

filter {
  if [type] == "hive.metastore.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hive.metastore.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hive-metastore-log"
    }
  }
}

#pipeline of hive server2 logs
input {
  file {
    path => "{{hive_log_dir}}/hiveserver2.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.server2.log"
    start_position => beginning
    type => "hive.server2.log"
  }
}

filter {
  if [type] == "hive.server2.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hive.server2.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hive-server2-log"
    }
  }
}

#pipeline of hive WebHCat Server logs
input {
  file {
    path => "{{webhcat_log_dir}}/webhcat.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.webhcat.log"
    start_position => beginning
    type => "hive.webhcat.log"
  }
}

filter {
  if [type] == "hive.webhcat.log" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
  }
}

output {
  if [type] == "hive.webhcat.log" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hive-webhcat-log"
    }
  }
}
    </value>
    <value-attributes>
      <show-property-name>false</show-property-name>
    </value-attributes>
  </property>
</configuration>