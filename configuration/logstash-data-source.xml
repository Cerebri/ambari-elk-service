<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->

<configuration>
  <!-- logstash.conf -->
  <property>
    <name>content</name>
    <description>This is logstash configuration file to define input, filter and output.</description>
    <value>
# pipeline of mapreduce finished jobs summary

input {
  file {
    path => "/var/log/hadoop-yarn/yarn/hadoop-mapreduce.jobsummary.log"
    sincedb_path => "{{logstash_sincedb_path}}"
    start_position => beginning
    type => "mapred"
  }
}

filter {
  grok {
    match => {   
      "message" => "\A%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+(?&lt;logger&gt;(?:[a-zA-Z0-9-]+\.)*[A-Za-z0-9$]+)\s*(:\s+)?appId=(?:%{WORD:appId}),name=(?&lt;name&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),user=(?&lt;user&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),queue=(?&lt;queue&gt;(?:[A-Za-z0-9]+(?:[ _-][A-Za-z0-9]+)*)),state=(?:%{WORD:state}),trackingUrl=(?&lt;trackingUrl&gt;(?:.*)),appMasterHost=(?&lt;appMasterHost&gt;(?:.*)),startTime=(?:%{WORD:startTime}),finishTime=(?:%{WORD:finishTime}),finalStatus=(?:%{WORD:finalStatus}),memorySeconds=(?:%{WORD:memorySeconds}),vcoreSeconds=(?:%{WORD:vcoreSeconds}),preemptedAMContainers=(?:%{WORD:preemptedAMContainers}),preemptedNonAMContainers=(?:%{WORD:preemptedNonAMContainers}),preemptedResources=&lt;memory:(?:%{WORD:preemptedResources_memory})(\\,.*)vCores:(?:%{WORD:preemptedResources_vCores})&gt;,applicationType=(?:%{WORD:applicationType})"
    }
  }
  mutate {
    convert => ["startTime", "integer"]
    convert => ["finishTime", "integer"]
    convert => ["memorySeconds", "integer"]
    convert => ["vcoreSeconds", "integer"]
    convert => ["preemptedAMContainers", "integer"]
    convert => ["preemptedNonAMContainers", "integer"]
    convert => ["preemptedResources_memory", "integer"]
    convert => ["preemptedResources_vCores", "integer"]
  }
  if [type] == "mapred" {
    ruby {
      code => "event['executionTime'] = event['finishTime']/1000 - event['startTime']/1000"
    }
  }
}

output {
  if [type] == "mapred" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-mapred"
    }
  }
}

#pipeline of hdfs datanode logs

input {
  file {
    path => "/var/log/hadoop/hdfs/hadoop-hdfs-datanode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}"
    start_position => beginning
    type => "hdfs"
  }
}

filter {}

output {
  if [type] == "hdfs" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs"
    }
  }
}
    </value>
    <value-attributes>
      <show-property-name>false</show-property-name>
    </value-attributes>
  </property>
</configuration>